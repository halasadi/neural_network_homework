{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "words = open('../external_resources/makemore/names.txt', 'r').read().splitlines()\n",
    "\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "xs_bi, ys_bi = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs_bi.append(ix1)\n",
    "    ys_bi.append(ix2)\n",
    "xs_bi = torch.tensor(xs_bi)\n",
    "ys_bi = torch.tensor(ys_bi)\n",
    "print('number of examples: ', xs_bi.nelement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-write the code from the lecture into a unified class to avoid code duplication\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NGramModel():\n",
    "    def __init__(self, xs, ys, vocab_size, output_dim, step_size = 50, shrinkage=0.01):\n",
    "\n",
    "        # data\n",
    "        self._xs = xs\n",
    "        self._ys = ys\n",
    "        self._num = xs.nelement()\n",
    "        self._vocab_size = vocab_size\n",
    "        self._output_dim = output_dim\n",
    "\n",
    "        # random number generators\n",
    "        self._g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "        # initialize parameters\n",
    "        self._W = torch.randn(self._vocab_size, self._output_dim, generator=self._g, requires_grad=True)\n",
    "\n",
    "        # hyperparameters\n",
    "        self._shrinkage = shrinkage\n",
    "        self._step_size = step_size\n",
    "        self._loss = None\n",
    "    \n",
    "    def _forward(self):\n",
    "        xenc = F.one_hot(self._xs, self._vocab_size).float()\n",
    "        logits = xenc @ self._W # predict log-counts\n",
    "        counts = logits.exp() # convert to counts\n",
    "        # (n x vocab_size) -> (vocab_size x 1)\n",
    "        # such that each entry is the probability of the corresponding character\n",
    "        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        # shirnking W to zero is equivalent to encouraging the network to predict uniform probabilities\n",
    "        loss = -probs[torch.arange(self._num), self._ys].log().mean() + self._shrinkage*(self._W**2).mean()\n",
    "        self._loss = loss\n",
    "\n",
    "    def _backward(self):\n",
    "        self._W.grad = None # set to zero the gradient\n",
    "        self._loss.backward()\n",
    "    \n",
    "    def _step(self):\n",
    "        self._W.data += -self._step_size * self._W.grad\n",
    "\n",
    "    def train(self, epochs = 200):\n",
    "        for k in range(epochs):\n",
    "\n",
    "            # compute loss\n",
    "            self._forward()\n",
    "\n",
    "            # compute gradients\n",
    "            self._backward()\n",
    "\n",
    "            # update parameters of the model\n",
    "            # using simple gradient descent\n",
    "            self._step()\n",
    "\n",
    "        print('loss: ', self._loss.item())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.4829957485198975\n"
     ]
    }
   ],
   "source": [
    "# train the bigram model to ensure we get the same loss as in the lecture\n",
    "model = NGramModel(xs_bi, ys_bi, 27, 27, step_size=50, shrinkage=0.01)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_mutated = [\"..\" + word + \".\" for word in words]\n",
    "\n",
    "# all unique bigrams\n",
    "unique_bigrams = sorted(list(set([w[i:i+2] for w in words_mutated for i in range(len(w)-1)])))\n",
    "stoi2 = {s:i for i,s in enumerate(unique_bigrams)}\n",
    "itos2 = {i:s for s,i in stoi2.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "xs_tri, ys_tri = [], []\n",
    "for w in words_mutated:\n",
    "  chs = list(w)\n",
    "  bigrams = [bi for bi in zip(chs, chs[1:])]\n",
    "  trigrams = zip(bigrams, chs[2:])\n",
    "  for (bigram, ch) in trigrams:\n",
    "    ix1 = stoi2[''.join(bigram)]\n",
    "    ix2 = stoi[ch]\n",
    "    xs_tri.append(ix1)\n",
    "    ys_tri.append(ix2)\n",
    "xs_tri = torch.tensor(xs_tri)\n",
    "ys_tri = torch.tensor(ys_tri)\n",
    "print('number of examples: ', xs_tri.nelement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tigram_model = NGramModel(xs_tri, ys_tri, len(unique_bigrams), 27, step_size=100, shrinkage=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.3098304271698\n"
     ]
    }
   ],
   "source": [
    "tigram_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "* E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram model\n",
    "\n",
    "n_train = int(0.8 * len(xs_bi))\n",
    "n_valid = int(0.1 * len(xs_bi))\n",
    "n_test = len(xs_bi) - n_train - n_valid # the rest\n",
    "trn_set_x_bi = xs_bi[:n_train]\n",
    "trn_set_y_bi = ys_bi[:n_train]\n",
    "val_set_x_bi = xs_bi[n_train:n_train+n_valid]\n",
    "val_set_y_bi = ys_bi[n_train:n_train+n_valid]\n",
    "tst_set_x_bi = xs_bi[n_train+n_valid:]\n",
    "tst_set_y_bi = ys_bi[n_train+n_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.4537971019744873\n"
     ]
    }
   ],
   "source": [
    "bigram_model = NGramModel(trn_set_x_bi, trn_set_y_bi, 27, 27, step_size=50, shrinkage=0.01)\n",
    "bigram_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x, y):\n",
    "    with torch.no_grad():\n",
    "        model._xs = x\n",
    "        model._ys = y\n",
    "        model._num = x.nelement()\n",
    "        model._forward()\n",
    "        loss = model._loss.item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  2.4537694454193115\n",
      "validation loss:  2.6190683841705322\n",
      "test loss:  2.62312388420105\n"
     ]
    }
   ],
   "source": [
    "print('train loss: ', compute_loss(bigram_model, trn_set_x_bi, trn_set_y_bi))\n",
    "print('validation loss: ', compute_loss(bigram_model, val_set_x_bi, val_set_y_bi))\n",
    "print('test loss: ', compute_loss(bigram_model, tst_set_x_bi, tst_set_y_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.2679600715637207\n",
      "train loss:  2.2675042152404785\n",
      "validation loss:  2.5189013481140137\n",
      "test loss:  2.5410876274108887\n"
     ]
    }
   ],
   "source": [
    "# trigram model\n",
    "n_train = int(0.8 * len(xs_tri))\n",
    "n_valid = int(0.1 * len(xs_tri))\n",
    "n_test = len(xs_tri) - n_train - n_valid # the rest\n",
    "trn_set_x_tri = xs_tri[:n_train]\n",
    "trn_set_y_tri = ys_tri[:n_train]\n",
    "val_set_x_tri = xs_tri[n_train:n_train+n_valid]\n",
    "val_set_y_tri = ys_tri[n_train:n_train+n_valid]\n",
    "tst_set_x_tri = xs_tri[n_train+n_valid:]\n",
    "tst_set_y_tri = ys_tri[n_train+n_valid:]\n",
    "\n",
    "trigram_model = NGramModel(trn_set_x_tri, trn_set_y_tri, len(unique_bigrams), 27, step_size=100, shrinkage=0.01)\n",
    "trigram_model.train()\n",
    "\n",
    "print('train loss: ', compute_loss(trigram_model, trn_set_x_tri, trn_set_y_tri))\n",
    "print('validation loss: ', compute_loss(trigram_model, val_set_x_tri, val_set_y_tri))\n",
    "print('test loss: ', compute_loss(trigram_model, tst_set_x_tri, tst_set_y_tri))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show that the trigram model still has a better loss, however, there is a substantially larger difference in loss between the test set and train set when using the trigram model. This makes sense, the trigram model has more parameters and thus it is easier to overfit to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "* E03: Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shrinkage:  0.0001\n",
      "loss:  2.2565741539001465\n",
      "shrinkage:  0.001\n",
      "loss:  2.257638931274414\n",
      "shrinkage:  0.01\n",
      "loss:  2.2679600715637207\n",
      "shrinkage:  0.1\n",
      "loss:  2.34486985206604\n",
      "shrinkage:  1.0\n",
      "loss:  2.5431833267211914\n"
     ]
    }
   ],
   "source": [
    "shrinkage_values = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "loss_values = []\n",
    "\n",
    "for shrinkage in shrinkage_values:\n",
    "    print('shrinkage: ', shrinkage)\n",
    "    trigram_model = NGramModel(trn_set_x_tri, trn_set_y_tri, len(unique_bigrams), 27, step_size=100, shrinkage=shrinkage)\n",
    "    trigram_model.train()\n",
    "    loss_values.append(compute_loss(trigram_model, val_set_x_tri, val_set_y_tri))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I probably did this wrong -- it seems like my loss increases as I increase the amount of shrinkage. I expected the opposite trend to hold true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 + 5\n",
    "\n",
    "* E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "* E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fast(x, y, W):\n",
    "    logits = W[x] # one-hot encoding\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return(loss)\n",
    "\n",
    "def forward_slow(x, y, W):\n",
    "    num = x.nelement()\n",
    "    xenc = F.one_hot(x, 27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # convert to counts\n",
    "    # (n x vocab_size) -> (vocab_size x 1)\n",
    "    # such that each entry is the probability of the corresponding character\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # shirnking W to zero is equivalent to encouraging the network to predict uniform probabilities\n",
    "    loss = -probs[torch.arange(num), y].log().mean() \n",
    "    return(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = NGramModel(trn_set_x_bi, trn_set_y_bi, 27, 27, step_size=50, shrinkage=0.01)\n",
    "W = bigram_model._W\n",
    "x = trn_set_x_bi\n",
    "y = trn_set_y_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7680, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_slow(x, y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7680, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_fast(x, y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
